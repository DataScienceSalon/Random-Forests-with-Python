---
title: "Random Forests with Python"
author: "John James jjames@datasciencesalon.org"
date: '`r format(Sys.time(), "%d %B, %Y")`'
output:
  rmdformats::readthedown:
    highlight: kate
    css: css/rmdStyles.css
    number_sections: false
editor_options: 
  chunk_output_type: inline
---


```{r knitr_init, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE}
options(knitr.table.format = "html")
options(max.print=100, scipen=999, width = 800)
knitr::opts_chunk$set(echo=FALSE,
	             cache=FALSE,
               prompt=FALSE,
	             eval = TRUE,
               tidy=TRUE,
               root.dir = "..",
               fig.height = 8,
               fig.width = 20,
               comment=NA,
               message=FALSE,
               warning=FALSE)
knitr::opts_knit$set(width=100, figr.prefix = T, figr.link = T)
knitr::knit_hooks$set(inline = function(x) {
  prettyNum(x, big.mark=",")
})
```

```{r r_libraries}
library(reticulate)
```


# Introduction
Random forests, a tree-based method which can be to applied both regression and classification problems, has a number of advantages over some more classical approaches, such as:
- Trees can be displayed graphically and are typically easier to interpret than most other models, including linear regression.
- Some believe that tree-based approaches more closely mirror human decision making than do more classical regression and classification methods.
- Decision trees implicitly perform variable screening or feature selection since the top few nodes on which the tree is split are invariably the most important variables within the dataset.  
- Trees require less data preprocessing than other methods as scaling, normalization, and the creation of dummy variables for qualitative predictors are not necessary. 

This vignette describes an application of random forests in python to a prediction problem in n-parts.
1. Business Casea
2. Data
2.1. Exploratory Data Analysis
2.2. Data Preprocessing
3. Introduction to Random Forests
3.1 Decision Trees
3.2 Bagging
3.3 Random Forests
4. Modeling
5. Prediction
